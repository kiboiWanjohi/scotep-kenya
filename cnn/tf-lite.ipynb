{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "! cp /content/drive/MyDrive/Stat_Docs/kaggle.json ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change permission\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle datasets download -d awsaf49/cbis-ddsm-breast-cancer-image-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check disk usage\n",
    "! df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import cv2\n",
    "import uuid\n",
    "import shutil\n",
    "import random\n",
    "import glob as gb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm  # Progress bar\n",
    "from scipy.special import gamma\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import Conv2D, MaxPool2D, BatchNormalization\n",
    "\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "# check correct file path on colab \n",
    "calc_train = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/calc_case_description_train_set.csv')\n",
    "calc_test = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/calc_case_description_test_set.csv')\n",
    "mass_train = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/mass_case_description_train_set.csv')\n",
    "mass_test = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/mass_case_description_test_set.csv')\n",
    "dicom_df = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/dicom_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_path(sample, old_path, new_path):\n",
    "    return sample.replace(old_path, new_path, regex=True)\n",
    "\n",
    "\n",
    "# if fxn above doesn't work try pd dataframe-type \n",
    "# to get \n",
    "# import pandas as pd\n",
    "\n",
    "# # Sample input (pandas Series)\n",
    "# data = pd.Series([\"/old/path/file1.jpg\", \"/old/path/file2.jpg\", \"/other/path/file3.jpg\"])\n",
    "\n",
    "# # Replace \"/old/path\" with \"/new/path\"\n",
    "# updated_data = replace_path(data, \"/old/path\", \"/new/path\")\n",
    "\n",
    "# print(updated_data)\n",
    "\n",
    "# 0    /new/path/file1.jpg\n",
    "# 1    /new/path/file2.jpg\n",
    "# 2    /other/path/file3.jpg\n",
    "# dtype: object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_smaples(sample, row=15, col=15):\n",
    "    plt.figure(figsize=(row, col))\n",
    "    for i, file in enumerate(sample[0:5]):\n",
    "#         PIL.Image.open(file): Opens the image file specified by file using the Python Imaging Library (PIL).\n",
    "# convert(\"L\"): Converts the image to grayscale (\"L\" mode), which reduces the color channels to shades of gray.\n",
    "        cropped_images_show = PIL.Image.open(file)\n",
    "        gray_img= cropped_images_show.convert(\"L\")\n",
    "        plt.subplot(1,5,i+1)\n",
    "        plt.imshow(gray_img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images = dicom_df[dicom_df.SeriesDescription==\"cropped images\"].image_path\n",
    "full_mammogram = dicom_df[dicom_df.SeriesDescription==\"full mammogram images\"].image_path\n",
    "roi_mask = dicom_df[dicom_df.SeriesDescription==\"ROI mask images\"].image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the path for cropped_images to the correct directory -- check on colab.\n",
    "correct_dir = \"../input/cbis-ddsm-breast-cancer-image-dataset/jpeg\"\n",
    "cropped_images = replace_path(cropped_images, \"CBIS-DDSM/jpeg\", correct_dir)\n",
    "print('Cropped Images paths:')\n",
    "print(cropped_images.iloc[0]) # Print to ensure everything looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the path for full_mammogram images to the correct directory.\n",
    "full_mammogram = replace_path(full_mammogram, \"CBIS-DDSM/jpeg\", correct_dir)\n",
    "print('\\nFull mammo Images paths:')\n",
    "print(full_mammogram.iloc[0]) # Print to ensure everything looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the path for roi_mask images to the correct directory.\n",
    "roi_mask = replace_path(roi_mask, \"CBIS-DDSM/jpeg\", correct_dir)\n",
    "print('\\nROI Mask Images paths:')\n",
    "print(roi_mask.iloc[0]) # Print to ensure everything looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find length of each dataset and ensure it matches -- all pictures are implemented as expected\n",
    "def get_image_file_name(data, new_dict):\n",
    "\n",
    "    for dicom in data:\n",
    "        key = dicom.split('/')[4]\n",
    "        new_dict[key] = dicom\n",
    "    print(f\"the length of dataset ==> {len(new_dict.keys())}\")\n",
    "\n",
    "cropped_images_dict = dict()\n",
    "full_mammo_dict = dict()\n",
    "roi_img_dict = dict()\n",
    "\n",
    "get_image_file_name(cropped_images, cropped_images_dict)\n",
    "get_image_file_name(full_mammogram, full_mammo_dict)\n",
    "get_image_file_name(roi_mask, roi_img_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the dicom paths to correct image paths\n",
    "def fix_image_path(data):\n",
    "    \"\"\"Correct dicom paths to correct image paths.\"\"\"\n",
    "    for indx, image in enumerate(data.values):\n",
    "        # Extract third component (index 2) of the path stored in column 11 (image[11]), \n",
    "        #assign resulting string to img_name.\n",
    "        img_name = image[11].split('/')[2]\n",
    "\n",
    "        # Check if img_name exists as a key in the dictionary full_mammo_dict.\n",
    "        # If exists, updates the value in column 11 of the current row (data.iloc[indx, 11]) with the corresponding value from the dictionary.\n",
    "        if img_name in full_mammo_dict:\n",
    "            data.iloc[indx, 11] = full_mammo_dict[img_name]\n",
    "        else:\n",
    "            data.iloc[indx, 11] = None\n",
    "        \n",
    "\n",
    "#         Extracts the third component (index 2) of the path stored in column 12 (image[12]).\n",
    "# Assigns the result to img_name.\n",
    "        img_name = image[12].split('/')[2]\n",
    "\n",
    "        # Checks if img_name exists as a key in the dictionary cropped_images_dict.If it exists, updates the value in column 12 of the current row (data.iloc[indx, 12]) with the corresponding value from the dictionary.\n",
    "        if img_name in cropped_images_dict:\n",
    "            data.iloc[indx, 12] = cropped_images_dict[img_name]\n",
    "        else:\n",
    "            data.iloc[indx, 11] = None\n",
    "\n",
    "\n",
    "        img_name = image[13].split('/')[2]\n",
    "        if img_name in roi_img_dict:\n",
    "            data.iloc[indx, 13] = roi_img_dict[img_name]\n",
    "\n",
    "        else:\n",
    "            data.iloc[indx, 13] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the function to fix image file paths for mass_train, mass_test, calc_train and calc_test\n",
    "fix_image_path(mass_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_train = mass_train.rename(columns={'left or right breast': 'left_or_right_breast',\n",
    "                                        'image view': 'image_view',\n",
    "                                        'abnormality id': 'abnormality_id',\n",
    "                                        'abnormality type': 'abnormality_type',\n",
    "                                        'mass shape': 'mass_shape',\n",
    "                                        'mass margins': 'mass_margins',\n",
    "                                        'image file path': 'image_file_path',\n",
    "                                        'cropped image file path': 'cropped_image_file_path',\n",
    "                                        'ROI mask file path': 'ROI_mask_file_path'})\n",
    "mass_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix image mass test\n",
    "fix_image_path(mass_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_test = mass_test.rename(columns={'left or right breast': 'left_or_right_breast',\n",
    "                                      'image view': 'image_view',\n",
    "                                      'abnormality id': 'abnormality_id',\n",
    "                                      'abnormality type': 'abnormality_type',\n",
    "                                      'mass shape': 'mass_shape',\n",
    "                                      'mass margins': 'mass_margins',\n",
    "                                      'image file path': 'image_file_path',\n",
    "                                      'cropped image file path': 'cropped_image_file_path',\n",
    "                                      'ROI mask file path': 'ROI_mask_file_path'})\n",
    "# view renamed columns\n",
    "mass_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_train = calc_train.rename(columns={'left or right breast': 'left_or_right_breast',\n",
    "                                        'image view': 'image_view',\n",
    "                                        'abnormality id': 'abnormality_id',\n",
    "                                        'abnormality type': 'abnormality_type',\n",
    "                                        'mass shape': 'mass_shape',\n",
    "                                        'mass margins': 'mass_margins',\n",
    "                                        'image file path': 'image_file_path',\n",
    "                                        'cropped image file path': 'cropped_image_file_path',\n",
    "                                        'ROI mask file path': 'ROI_mask_file_path'})\n",
    "# view renamed columns\n",
    "calc_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_image_path(calc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_test = calc_test.rename(columns={'left or right breast': 'left_or_right_breast',\n",
    "                                      'image view': 'image_view',\n",
    "                                      'abnormality id': 'abnormality_id',\n",
    "                                      'abnormality type': 'abnormality_type',\n",
    "                                      'mass shape': 'mass_shape',\n",
    "                                      'mass margins': 'mass_margins',\n",
    "                                      'image file path': 'image_file_path',\n",
    "                                      'cropped image file path': 'cropped_image_file_path',\n",
    "                                      'ROI mask file path': 'ROI_mask_file_path'})\n",
    "# view renamed columns\n",
    "calc_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_image_path(calc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (1798544137.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    for index, (i, row) in enumerate()\u001b[0m\n\u001b[1;37m                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "# Display images function \n",
    "def display_images(dataset, column, number):\n",
    "    fig, axes = plt.subplots(1, number, figsize=(15,5))\n",
    "    for index, (i, row) in enumerate(dataset.head(number).iterrows()):\n",
    "                image_path = row[column]\n",
    "                # check for valid path\n",
    "                if image_path is None or not os.path.exists(image_path):\n",
    "                        # print(f\"File not found or invalid path: {image_path}\")\n",
    "                        continue\n",
    "                # read image (BGR format)\n",
    "                image = cv2.imread(image_path)\n",
    "                # handle image read errors\n",
    "                if image is None:\n",
    "                        # print(f\"Eror reading image: {image_path})\n",
    "                        continue\n",
    "                # convert bgr to rgb\n",
    "                if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "                        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                # assign subplot index to ax \n",
    "                ax = axes[index]\n",
    "                ax.imshow(image, cmap='gray' if len(image.shape) == 2 else None)\n",
    "                ax.set_title(f\"{row['pathology']}\")\n",
    "                ax.axis('off')\n",
    "\n",
    "                # print image shape \n",
    "                print(np.array(image).shape)\n",
    "\n",
    "                # finalize and display\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Full Mammograms:\\n')\n",
    "display_images(mass_train, 'image_file_path', 5)\n",
    "print('Cropped Mammograms:\\n')\n",
    "display_images(mass_train, 'cropped_image_file_path', 5)\n",
    "print('ROI_mask:\\n')\n",
    "display_images(mass_train, 'ROI_mask_file_path', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Full Mammograms:\\n')\n",
    "display_images(mass_test, 'image_file_path', 5)\n",
    "print('Cropped Mammograms:\\n')\n",
    "display_images(mass_test, 'cropped_image_file_path', 5)\n",
    "print('ROI_mask:\\n')\n",
    "display_images(mass_test, 'ROI_mask_file_path', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Full Mammograms:\\n')\n",
    "display_images(calc_train, 'image_file_path', 5)\n",
    "print('Cropped Mammograms:\\n')\n",
    "display_images(calc_train, 'cropped_image_file_path', 5)\n",
    "print('ROI_mask:\\n')\n",
    "display_images(calc_train, 'ROI_mask_file_path', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Full Mammograms:\\n')\n",
    "display_images(calc_test, 'image_file_path', 5)\n",
    "print('Cropped Mammograms:\\n')\n",
    "display_images(calc_test, 'cropped_image_file_path', 5)\n",
    "print('ROI_mask:\\n')\n",
    "display_images(calc_test, 'ROI_mask_file_path', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine both datasets to one \n",
    "full_dataset = pd.concat([calc_train, calc_test], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treat  BENIGN_WITHOUT_CALLBACK as 0\n",
    "class_mapper = {'MALIGNANT': 1, 'BENIGN': 0, 'BENIGN_WITHOUT_CALLBACK': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224, 224, 3)\n",
    "\n",
    "# apply class mapper to pathology column \n",
    "full_dataset['labels'] = full_dataset['pathology'].replace(class_mapper).infer_objects(copy=False)\n",
    "\n",
    "full_images = np.array(full_dataset[full_dataset[\"image_file_path\"].notna()]['image_file_path'].tolist())\n",
    "\n",
    "full_labels = np.array(full_dataset[full_dataset['image_file_path'].notna()]['labels'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ful_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert full_labels from numpy arra to pandas as series\n",
    "full_labels_series = pd.Series(full_labels)\n",
    "\n",
    "# count occurences of ech class\n",
    "label_counts = full_labels_series.value.counts()\n",
    "\n",
    "benign_count = label_counts.get(0, 0)\n",
    "malignant_count = label_counts.get(1, 0)\n",
    "\n",
    "print(f\"Benign IMages: {benign_count}\")\n",
    "print(f\"Malignant IMages: {malignant_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(full_dataset['labels'].unique())\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Benign', 'Malignant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of labels\n",
    "label_counts = full_dataset['labels'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation \n",
    "def augment_image(image):\n",
    "    # Apply data augmentation using tf.image functions\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "#     image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.3)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "    return image\n",
    "\n",
    "# Function to resize image to (224, 224, 3)\n",
    "def resize_image(image_tensor):\n",
    "    return tf.image.resize(image_tensor, [224, 224])\n",
    "\n",
    "# Function to balance classes by augmenting images\n",
    "def copy_images_with_unique_filenames(images, labels, source, destination, target_count=None):\n",
    "    \"\"\"\n",
    "    Copy images from source to destination in subfolders '0' and '1',\n",
    "    ensuring unique filenames and applying data augmentation and balancing.\n",
    "    \"\"\"\n",
    "    benign_images = 0\n",
    "    malignant_images = 0\n",
    "    skipped_images = []\n",
    "\n",
    "    # Create the destination subfolders '0' and '1'\n",
    "    category_dest_dir_zero = os.path.join(destination, '0')\n",
    "    os.makedirs(category_dest_dir_zero, exist_ok=True)\n",
    "\n",
    "    category_dest_dir_one = os.path.join(destination, '1')\n",
    "    os.makedirs(category_dest_dir_one, exist_ok=True)\n",
    "\n",
    "    benign_images_list = []\n",
    "    malignant_images_list = []\n",
    "\n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "#         img_name = data_frame.REFNUM[i]\n",
    "#         abs_path = os.path.join(source, img_name + '.pgm')\n",
    "\n",
    "        if os.path.exists(image):\n",
    "            try:\n",
    "                # Generate a unique filename\n",
    "                filename = os.path.basename(image)\n",
    "                unique_filename = f\"{uuid.uuid4().hex}_{filename}\"\n",
    "        \n",
    "                # Open the image using PIL\n",
    "                with Image.open(image) as img:\n",
    "                    # Convert the image to RGB mode (for saving as JPEG)\n",
    "                    img = img.convert('RGB')\n",
    "                    # Augment the image (convert it to a Tensor first)\n",
    "                    img_tensor = tf.convert_to_tensor(img)\n",
    "                    # Resize the image to (224, 224, 3)\n",
    "                    resized_img_tensor = resize_image(img_tensor)\n",
    "                    augmented_image_tensor = augment_image(resized_img_tensor)\n",
    "                    # Convert Tensor back to PIL image for saving\n",
    "                    augmented_image = tf.keras.preprocessing.image.array_to_img(augmented_image_tensor)\n",
    "\n",
    "                    if label == 0:\n",
    "                        benign_images_list.append(unique_filename)\n",
    "                        dest_path = os.path.join(category_dest_dir_zero, unique_filename)\n",
    "#                         augmented_image.save(dest_path, 'JPEG')\n",
    "                        augmented_image.save(dest_path, 'JPEG')\n",
    "                        benign_images += 1\n",
    "\n",
    "                    elif label == 1:\n",
    "                        malignant_images_list.append(unique_filename)\n",
    "                        dest_path = os.path.join(category_dest_dir_one, unique_filename)\n",
    "#                         augmented_image.save(dest_path, 'JPEG')\n",
    "                        augmented_image.save(dest_path, 'JPEG')\n",
    "                        malignant_images += 1\n",
    "                        \n",
    "#                 del img, img_tensor, resized_img_tensor, augmented_image_tensor, augmented_image\n",
    "#                 gc.collect()\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying image {image}: {e}\")\n",
    "                skipped_images.append(image)\n",
    "        else:\n",
    "            print(f\"Image not found: {image}\")\n",
    "            skipped_images.append(image)\n",
    "\n",
    "    # If balancing is needed, duplicate/augment images from the smaller class\n",
    "    benign_count = len(benign_images_list)\n",
    "    malignant_count = len(malignant_images_list)\n",
    "\n",
    "    if benign_count < malignant_count:\n",
    "#         augment_and_save_images(benign_images_list, category_dest_dir_zero, target_count - benign_count)\n",
    "        augment_and_save_images(benign_images_list, category_dest_dir_zero, malignant_count - benign_count)\n",
    "\n",
    "    elif malignant_count < benign_count:\n",
    "        augment_and_save_images(malignant_images_list, category_dest_dir_one, benign_count - malignant_count)\n",
    "\n",
    "    augment_and_save_images(benign_images_list, category_dest_dir_zero, target_count)\n",
    "    augment_and_save_images(malignant_images_list, category_dest_dir_one, target_count)\n",
    "\n",
    "    print(f\"\\nCopying complete.\")\n",
    "    print(f\"Benign images copied (label 0): {benign_images}\")\n",
    "    print(f\"Benign count (label 0): {benign_count}\")\n",
    "    print(f\"Malignant images copied (label 1): {malignant_images}\")\n",
    "    print(f\"Malignant count (label 1): {malignant_count}\")\n",
    "    print(f\"Total skipped images: {len(skipped_images)}\")\n",
    "    if skipped_images:\n",
    "        print(\"Skipped images:\")\n",
    "        for img in skipped_images:\n",
    "            print(img)\n",
    "            \n",
    "\n",
    "# Function to augment and save images to balance the dataset\n",
    "def augment_and_save_images(images_list, destination_dir, num_augments):\n",
    "    \"\"\"\n",
    "    Augment and save images to balance the dataset.\n",
    "    \"\"\"\n",
    "    for i in range(num_augments):\n",
    "        img_name = random.choice(images_list)\n",
    "        abs_path = os.path.join(destination_dir, img_name)\n",
    "\n",
    "        try:\n",
    "            with Image.open(abs_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                # Augment the image\n",
    "                img_tensor = tf.convert_to_tensor(img)\n",
    "                # Resize the image\n",
    "#                 resized_img_tensor = resize_image(img_tensor)\n",
    "                augmented_image_tensor = augment_image(img_tensor)\n",
    "                # Convert Tensor back to PIL image for saving\n",
    "                augmented_image = tf.keras.preprocessing.image.array_to_img(augmented_image_tensor)\n",
    "                # Remove the original extension from img_name 1-285.jpg --> 1-285\n",
    "                img_name_without_ext = os.path.splitext(img_name)[0]\n",
    "                # Save augmented image with a unique name\n",
    "                augmented_image.save(os.path.join(destination_dir, img_name_without_ext + f'_aug{i}.jpg'), 'JPEG')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error augmenting image {abs_path}: {e}\")\n",
    "\n",
    "source_dir = \"/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/jpeg\"\n",
    "destination_dir = \"/kaggle/working/merged_images\"\n",
    "\n",
    "# target_count=0 meaning no Augmentation, There's just Data-Balance\n",
    "target_count = (len(full_labels) * 3) - len(full_labels)\n",
    "copy_images_with_unique_filenames(full_images, full_labels, source_dir, destination_dir, target_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of images in each class folder after merging \n",
    "zero_class_count = len(os.listdir(\"kaggle/working/merged_images/0\"))\n",
    "one_class_count = len(os.listdir(\"kaggle.working/merged_images/1\"))\n",
    "\n",
    "print(f\"Number of images in class 0: {zero_class_count}\")\n",
    "print(f\"Number of images in class 1: {one_class_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "data_dir = '/kaggle/working/merged_images'  # Update with the dataset path\n",
    "\n",
    "# Create a dataset for the entire data to use for split\n",
    "full_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    # image_size=(224, 224),\n",
    "    image_size=(224, 224),\n",
    "    seed=50,\n",
    "    shuffle=True,\n",
    "    batch_size=13\n",
    ")\n",
    "# Calculate the total number of samples\n",
    "total_samples = tf.data.experimental.cardinality(full_dataset).numpy()\n",
    "\n",
    "train_size = int(0.8 * total_samples)                 # 70% for training\n",
    "val_size   = int(0.15 * total_samples)                # 20% for validation\n",
    "test_size = total_samples - train_size - val_size     # 10% for testing\n",
    "\n",
    "# Create train, validation, and test datasets\n",
    "train_dataset       = full_dataset.take(train_size)\n",
    "validation_dataset  = full_dataset.skip(train_size).take(val_size)\n",
    "test_dataset        = full_dataset.skip(train_size + val_size)\n",
    "\n",
    "train_dataset      = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset       = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Print the number of samples in each dataset\n",
    "print(f\"Train samples:      {train_size}     batches(13) ==> {train_size*13}\")\n",
    "print(f\"Validation samples: {val_size}       batches(13) ==> {val_size*13}\")\n",
    "print(f\"Test samples:       {test_size}      batches(13) ==> {test_size*13}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Builds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "def try_model():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # Freeze all layers initially\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Calculate the index to start unfreezing layers\n",
    "    from_index = int(np.round((len(base_model.layers) - 1) * (1.0 - 50.0 / 100.0)))\n",
    "\n",
    "    # Unfreeze layers from the calculated index onwards\n",
    "    for layer in base_model.layers[from_index:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Add custom layers on top (Upper-Layers)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(2, activation='softmax')(x)  # Assuming binary classification\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # # Clear the base model from memory if needed (optional)\n",
    "    # del model_dict, base_model, from_index, x, predictions;    gc.collect()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "trymodel = try_model()\n",
    "\n",
    "trymodel.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])  # Compile the model\n",
    "trymodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trymodel.fit(\n",
    "            train_dataset,\n",
    "            validation_data=validation_dataset,\n",
    "            batch_size=13,\n",
    "            epochs=7\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd Model\n",
    "# Define the second model with adjusted hyperparameters\n",
    "def try_model_v2():\n",
    "    base_model_v2 = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # Freeze all layers initially\n",
    "    for layer in base_model_v2.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Unfreeze 60% of layers instead of 50%\n",
    "    from_index_v2 = int(np.round((len(base_model_v2.layers) - 1) * (1.0 - 60.0 / 100.0)))\n",
    "\n",
    "    # Unfreeze layers from the calculated index onwards\n",
    "    for layer in base_model_v2.layers[from_index_v2:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Add custom layers on top (Upper-Layers)\n",
    "    x_v2 = base_model_v2.output\n",
    "    x_v2 = GlobalAveragePooling2D()(x_v2)\n",
    "    x_v2 = Dense(512, activation='relu')(x_v2)  # Reduced Dense layer size from 1024 to 512\n",
    "    x_v2 = Dropout(0.3)(x_v2)  # Lowered dropout rate from 0.5 to 0.3\n",
    "    predictions_v2 = Dense(2, activation='softmax')(x_v2)  # Assuming binary classification\n",
    "\n",
    "    model_v2 = Model(inputs=base_model_v2.input, outputs=predictions_v2)\n",
    "    \n",
    "    return model_v2\n",
    "\n",
    "# Create and compile the second model\n",
    "trymodel_v2 = try_model_v2()\n",
    "trymodel_v2.compile(optimizer=Adam(learning_rate=5e-5),  # Adjusted learning rate to 5e-5\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy', Precision(name='precision_v2'), Recall(name='recall_v2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the second model\n",
    "history_v2 = trymodel_v2.fit(\n",
    "            train_dataset,\n",
    "            validation_data=validation_dataset,\n",
    "            batch_size=13,\n",
    "            epochs=7\n",
    "        )\n",
    "\n",
    "# Summarize the second model\n",
    "trymodel_v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Model\n",
    "# Define the third model with different hyperparameters\n",
    "def try_model_v3():\n",
    "    base_model_v3 = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # Freeze all layers initially\n",
    "    for layer in base_model_v3.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Unfreeze 70% of layers (more layers unfreezed compared to v1 and v2)\n",
    "    from_index_v3 = int(np.round((len(base_model_v3.layers) - 1) * (1.0 - 70.0 / 100.0)))\n",
    "\n",
    "    # Unfreeze layers from the calculated index onwards\n",
    "    for layer in base_model_v3.layers[from_index_v3:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Add custom layers on top (Upper-Layers)\n",
    "    x_v3 = base_model_v3.output\n",
    "    x_v3 = GlobalAveragePooling2D()(x_v3)\n",
    "    x_v3 = Dense(1024, activation='relu')(x_v3)  # Return Dense layer size to 1024\n",
    "    x_v3 = Dropout(0.4)(x_v3)  # Moderate dropout rate (between v1 and v2)\n",
    "    predictions_v3 = Dense(2, activation='softmax')(x_v3)  # Assuming binary classification\n",
    "\n",
    "    model_v3 = Model(inputs=base_model_v3.input, outputs=predictions_v3)\n",
    "    \n",
    "    return model_v3\n",
    "\n",
    "# Create and compile the third model\n",
    "trymodel_v3 = try_model_v3()\n",
    "trymodel_v3.compile(optimizer=RMSprop(learning_rate=1e-4),  # Change optimizer to RMSprop and learning rate back to 1e-4\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy', Precision(name='precision_v3'), Recall(name='recall_v3')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the third model\n",
    "history_v3 = trymodel_v3.fit(\n",
    "            train_dataset,\n",
    "            validation_data=validation_dataset,\n",
    "            batch_size=13,\n",
    "            epochs=7\n",
    "        )\n",
    "\n",
    "# Summarize the third model\n",
    "trymodel_v3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comaprison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Model 1 Accuracy')\n",
    "plt.plot(history_v2.history['accuracy'], label='Model 2 Accuracy')\n",
    "plt.plot(history_v3.history['accuracy'], label='Model 3 Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Validation Accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['val_accuracy'], label='Model 1 Val Accuracy')\n",
    "plt.plot(history_v2.history['val_accuracy'], label='Model 2 Val Accuracy')\n",
    "plt.plot(history_v3.history['val_accuracy'], label='Model 3 Val Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(history.history['loss'], label='Model 1 Loss')\n",
    "plt.plot(history_v2.history['loss'], label='Model 2 Loss')\n",
    "plt.plot(history_v3.history['loss'], label='Model 3 Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Validation Loss\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(history.history['val_loss'], label='Model 1 Val Loss')\n",
    "plt.plot(history_v2.history['val_loss'], label='Model 2 Val Loss')\n",
    "plt.plot(history_v3.history['val_loss'], label='Model 3 Val Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['precision'], label='Model 1 Precision')\n",
    "plt.plot(history_v2.history['precision_v2'], label='Model 2 Precision')\n",
    "plt.plot(history_v3.history['precision_v3'], label='Model 3 Precision')\n",
    "plt.title('Training Precision')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['val_precision'], label='Model 1 Val Precision')\n",
    "plt.plot(history_v2.history['val_precision_v2'], label='Model 2 Val Precision')\n",
    "plt.plot(history_v3.history['val_precision_v3'], label='Model 3 Val Precision')\n",
    "plt.title('Validation Precision')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Recall\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['recall'], label='Model 1 Recall')\n",
    "plt.plot(history_v2.history['recall_v2'], label='Model 2 Recall')\n",
    "plt.plot(history_v3.history['recall_v3'], label='Model 3 Recall')\n",
    "plt.title('Training Recall')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['val_recall'], label='Model 1 Val Recall')\n",
    "plt.plot(history_v2.history['val_recall_v2'], label='Model 2 Val Recall')\n",
    "plt.plot(history_v3.history['val_recall_v3'], label='Model 3 Val Recall')\n",
    "plt.title('Validation Recall')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics for each model's history\n",
    "metrics_data = {\n",
    "    'Model': [],\n",
    "    'Epoch': [],\n",
    "    'Training Accuracy': [],\n",
    "    'Validation Accuracy': [],\n",
    "    'Training Loss': [],\n",
    "    'Validation Loss': []\n",
    "}\n",
    "\n",
    "# Model 1 (history)\n",
    "for epoch in range(len(history.history['accuracy'])):\n",
    "    metrics_data['Model'].append('Model 1')\n",
    "    metrics_data['Epoch'].append(epoch + 1)\n",
    "    metrics_data['Training Accuracy'].append(history.history['accuracy'][epoch])\n",
    "    metrics_data['Validation Accuracy'].append(history.history['val_accuracy'][epoch])\n",
    "    metrics_data['Training Loss'].append(history.history['loss'][epoch])\n",
    "    metrics_data['Validation Loss'].append(history.history['val_loss'][epoch])\n",
    "\n",
    "# Model 2 (history_v2)\n",
    "for epoch in range(len(history_v2.history['accuracy'])):\n",
    "    metrics_data['Model'].append('Model 2')\n",
    "    metrics_data['Epoch'].append(epoch + 1)\n",
    "    metrics_data['Training Accuracy'].append(history_v2.history['accuracy'][epoch])\n",
    "    metrics_data['Validation Accuracy'].append(history_v2.history['val_accuracy'][epoch])\n",
    "    metrics_data['Training Loss'].append(history_v2.history['loss'][epoch])\n",
    "    metrics_data['Validation Loss'].append(history_v2.history['val_loss'][epoch])\n",
    "\n",
    "# Model 3 (history_v3)\n",
    "for epoch in range(len(history_v3.history['accuracy'])):\n",
    "    metrics_data['Model'].append('Model 3')\n",
    "    metrics_data['Epoch'].append(epoch + 1)\n",
    "    metrics_data['Training Accuracy'].append(history_v3.history['accuracy'][epoch])\n",
    "    metrics_data['Validation Accuracy'].append(history_v3.history['val_accuracy'][epoch])\n",
    "    metrics_data['Training Loss'].append(history_v3.history['loss'][epoch])\n",
    "    metrics_data['Validation Loss'].append(history_v3.history['val_loss'][epoch])\n",
    "\n",
    "# Create a DataFrame from the metrics data\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Display the metrics table\n",
    "print(metrics_df)\n",
    "\n",
    "# Save the metrics table to a CSV file\n",
    "metrics_df.to_csv('model_metrics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
